import warnings
warnings.filterwarnings('ignore')  # ê²½ê³  ë©”ì‹œì§€ ë¬´ì‹œ
import logging
logging.basicConfig(level=logging.INFO)  # ë©”ì¸íŒŒì¼ì—ì„œ 2ê°œì¤„ ë‹¤ ì“°ë©´ ë‹¤ë¥¸ íŒŒì¼ì—ì„œëŠ” ì²«ë²ˆì§¸ 
## ë¡œê¹… ë ˆë²¨ ì„¤ì •. ë¡œê¹…ì´ë€ ì½”ë“œ ì‹¤í–‰ ì¤‘ ë°œìƒí•˜ëŠ” ì´ë²¤íŠ¸ë¥¼ ê¸°ë¡í•˜ëŠ” ê²ƒ. print()ë³´ë‹¤ ê¹”ë”í•˜ê³  ë©”ëª¨ë¦¬ë‚­ë¹„ê°€ ì—†ìŒ. í˜„ì—…ì—ì„œ ë§ì´ ì‚¬ìš©í•¨.
## INFO : logging.info() ì¶œë ¥, ERROR : logging.info()ê°€ ì¶œë ¥ë˜ì§€ ì•Šê²Œ í•¨

import argparse # í„°ë¯¸ë„ì—ì„œ ì¸ìë¥¼ ì…ë ¥í•˜ëŠ”ëŒ€ë¡œ ì¶œë ¥í•˜ê²Œ í•˜ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬
import time
from datetime import datetime

##########
# ì½”ë“œ ì •ì˜ ì˜ì—­
##########
# import ë’¤ì—ëŠ” í•¨ìˆ˜ ë˜ëŠ” í´ë˜ìŠ¤ë§Œ ì‘ì„±
# from ë’¤ì—ëŠ” í´ë”.íŒŒì¼ëª… ì‘ì„±

from service.data_setup import do_load_dataset
from service.preprocessing.adata_preprocessing import do_preprocessing
from service.modeling.training import do_training
from service.modeling.model import Model_Type
import json
import os
# from service.submission import create_submission_file

def create_final_comparison_report(successful_models):
    """ëª¨ë“  ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ë¹„êµí•˜ëŠ” ìµœì¢… ë³´ê³ ì„œ ìƒì„±"""
    
    model_performances = []
    
    # ê° ëª¨ë¸ì˜ ë³´ê³ ì„œì—ì„œ ì„±ëŠ¥ ë°ì´í„° ìˆ˜ì§‘
    for model_name in successful_models:
        report_file = f"reports/{model_name}_report.json"
        if os.path.exists(report_file):
            with open(report_file, 'r', encoding='utf-8') as f:
                report = json.load(f)
                
            model_performances.append({
                'model_name': model_name,
                'test_f1_score': report['test_performance']['test_f1_score'],
                'test_accuracy': report['test_performance']['test_accuracy'],
                'test_roc_auc': report['test_performance']['test_roc_auc'],
                'cv_f1_score': report['cross_validation']['cv_f1_score'],
                'cv_accuracy': report['cross_validation']['cv_accuracy'],
                'cv_roc_auc': report['cross_validation']['cv_roc_auc'],
                'training_time': report['model_details']['training_time']
            })
    
    # F1 ìŠ¤ì½”ì–´ ê¸°ì¤€ìœ¼ë¡œ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬
    model_performances.sort(key=lambda x: x['test_f1_score'], reverse=True)
    
    # ìµœì¢… ë¹„êµ ë³´ê³ ì„œ ìƒì„±
    final_report = {
        "report_type": "final_model_comparison",
        "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "total_models": len(model_performances),
        "ranking_criteria": "test_f1_score",
        "model_rankings": []
    }
    
    # ìˆœìœ„ë³„ ëª¨ë¸ ì •ë³´ ì¶”ê°€
    for rank, model_perf in enumerate(model_performances, 1):
        final_report["model_rankings"].append({
            "rank": rank,
            "model_name": model_perf['model_name'],
            "test_f1_score": model_perf['test_f1_score'],
            "test_accuracy": model_perf['test_accuracy'], 
            "test_roc_auc": model_perf['test_roc_auc'],
            "cv_f1_score": model_perf['cv_f1_score'],
            "training_time": model_perf['training_time']
        })
    
    # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì •ë³´
    if model_performances:
        best_model = model_performances[0]
        final_report["best_model"] = {
            "name": best_model['model_name'],
            "test_f1_score": best_model['test_f1_score'],
            "improvement_over_worst": round(best_model['test_f1_score'] - model_performances[-1]['test_f1_score'], 4) if len(model_performances) > 1 else 0
        }
    
    # JSON íŒŒì¼ë¡œ ì €ì¥
    final_report_file = "reports/final_model_comparison.json"
    with open(final_report_file, 'w', encoding='utf-8') as f:
        json.dump(final_report, f, indent=2, ensure_ascii=False)
    
    # í‘œ í˜•íƒœë¡œ ì½˜ì†” ì¶œë ¥
    logging.info("="*60)
    logging.info("ğŸ† ìµœì¢… ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ (F1-Score ê¸°ì¤€)")
    logging.info("="*60)
    logging.info("Model                     F1-Score   Accuracy   ROC-AUC    Time      Type")
    logging.info("="*84)
    
    # ê° ëª¨ë¸ë³„ë¡œ CV, Test, Diff 3ì¤„ì”© ì¶œë ¥
    for rank, model_perf in enumerate(model_performances, 1):
        model_name = model_perf['model_name'].capitalize()
        
        # CV ì„±ëŠ¥
        logging.info(f"{model_name:<25} {model_perf['cv_f1_score']:<10.4f} {model_perf['cv_accuracy']:<10.4f} {model_perf['cv_roc_auc']:<10.4f} {model_perf['training_time']:<9} CV")
        
        # Test ì„±ëŠ¥  
        logging.info(f"{'':25} {model_perf['test_f1_score']:<10.4f} {model_perf['test_accuracy']:<10.4f} {model_perf['test_roc_auc']:<10.4f} {'-':<9} Test")
        
        # ì°¨ì´ê°’ (Test - CV)
        f1_diff = model_perf['test_f1_score'] - model_perf['cv_f1_score']
        acc_diff = model_perf['test_accuracy'] - model_perf['cv_accuracy']
        auc_diff = model_perf['test_roc_auc'] - model_perf['cv_roc_auc']
        
        logging.info(f"{'':25} {f1_diff:<10.4f} {acc_diff:<10.4f} {auc_diff:<10.4f} {'-':<9} Diff")
        logging.info("-"*84)
    
    # CV-Test ì°¨ì´ ë¶„ì„ (ì¼ë°˜í™” ì„±ëŠ¥)
    logging.info("")
    logging.info("="*60)
    logging.info("ğŸ“Š CV-Test ì„±ëŠ¥ ì°¨ì´ ë¶„ì„ (ì¼ë°˜í™” ì„±ëŠ¥)")
    logging.info("="*60)
    logging.info("Rank  Model               F1_Diff    Acc_Diff   AUC_Diff   Overall_Diff")
    logging.info("="*84)
    
    # ì¼ë°˜í™” ì„±ëŠ¥ ìˆœìœ¼ë¡œ ì •ë ¬ (ì°¨ì´ê°€ ì‘ì„ìˆ˜ë¡ ì¢‹ìŒ)
    generalization_ranking = []
    for model_perf in model_performances:
        f1_diff = model_perf['test_f1_score'] - model_perf['cv_f1_score']
        acc_diff = model_perf['test_accuracy'] - model_perf['cv_accuracy']
        auc_diff = model_perf['test_roc_auc'] - model_perf['cv_roc_auc']
        overall_diff = (abs(f1_diff) + abs(acc_diff) + abs(auc_diff)) / 3
        
        generalization_ranking.append({
            'model_name': model_perf['model_name'],
            'f1_diff': f1_diff,
            'acc_diff': acc_diff,
            'auc_diff': auc_diff, 
            'overall_diff': overall_diff
        })
    
    # ì¼ë°˜í™” ì„±ëŠ¥ ìˆœìœ¼ë¡œ ì •ë ¬ (ì°¨ì´ê°€ ì‘ì„ìˆ˜ë¡ ìƒìœ„)
    generalization_ranking.sort(key=lambda x: abs(x['overall_diff']))
    
    for rank, gen_perf in enumerate(generalization_ranking, 1):
        medal = "ğŸ†" if rank == 1 else ""
        logging.info(f"{rank}ìœ„   {gen_perf['model_name'].capitalize():<15} {gen_perf['f1_diff']:<10.4f} {gen_perf['acc_diff']:<10.4f} {gen_perf['auc_diff']:<10.4f} {gen_perf['overall_diff']:<10.4f} {medal}")
    
    logging.info("="*84)
    
    # ì¢…í•© ë¶„ì„
    logging.info("")
    logging.info("="*60)
    logging.info("ğŸ¯ ì¢…í•© ë¶„ì„ ê²°ê³¼")
    logging.info("="*60)
    
    if model_performances:
        best_performance = model_performances[0]
        fastest_model = min(model_performances, key=lambda x: float(x['training_time'].replace(' seconds', '')))
        most_stable = generalization_ranking[0]
        avg_diff = sum([g['overall_diff'] for g in generalization_ranking]) / len(generalization_ranking)
        
        logging.info(f"ğŸ¥‡ ìµœê³  í…ŒìŠ¤íŠ¸ ì„±ëŠ¥:     {best_performance['model_name'].upper():<12} (F1: {best_performance['test_f1_score']:.4f})")
        logging.info(f"âš¡ ê°€ì¥ ë¹ ë¥¸ í•™ìŠµ:       {fastest_model['model_name'].upper():<12} (Time: {fastest_model['training_time']})")
        logging.info(f"ğŸ¨ ê°€ì¥ ì•ˆì •ì  ëª¨ë¸:     {most_stable['model_name'].upper():<12} (Overall Diff: {most_stable['overall_diff']:.4f})")
        logging.info(f"ğŸ“ˆ í‰ê·  ì¼ë°˜í™” ì°¨ì´:     {avg_diff:.4f}      (CV-Test ì„±ëŠ¥ ì°¨ì´)")
        
        logging.info("")
        logging.info("="*60)
        logging.info(f"ğŸ† ìµœì¢… ì¶”ì²œ ëª¨ë¸: {best_performance['model_name'].upper()}")
        logging.info(f"   âœ… ì´ìœ : ìµœê³  í…ŒìŠ¤íŠ¸ ì„±ëŠ¥ + ìƒëŒ€ì  ì•ˆì •ì„±")
        logging.info(f"   ğŸ“Š Test F1-Score: {best_performance['test_f1_score']:.4f}")
        logging.info(f"   ğŸ¯ CV-Test ì°¨ì´: {most_stable['overall_diff']:.4f}")
        logging.info("="*60)
        
    logging.info(f"ğŸ“Š ìµœì¢… ë¹„êµ ë³´ê³ ì„œ ì €ì¥: {final_report_file}")
    logging.info("="*60)
    
    return final_report

def main(args):
    # ì „ì²´ ì‹¤í–‰ ì‹œì‘ ì‹œê°„
    total_start_time = time.time()
    
    logging.info("="*50)
    logging.info("ğŸ¨ í˜¸í…” ì˜ˆì•½ ì·¨ì†Œ ì˜ˆì¸¡ ëª¨ë¸ - ì „ì²´ ëª¨ë¸ ì‹¤í–‰ ì‹œì‘")
    logging.info(f"ì‹¤í–‰ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    logging.info("="*50)
    
    # 1. ë°ì´í„° ë¡œë“œ (í•œ ë²ˆë§Œ ìˆ˜í–‰)
    logging.info("ğŸ“Š ë°ì´í„° ë¡œë“œ ì‹œì‘...")
    data_start_time = time.time()
    x_tr, x_te, y_tr, y_te = do_load_dataset(
        train_path=args.path_train, test_path=args.path_test
        , target_name=args.target_name)
    data_time = time.time() - data_start_time
    logging.info(f"âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ (ì†Œìš”ì‹œê°„: {data_time:.2f}ì´ˆ)")

    # 2. ë°ì´í„° ì „ì²˜ë¦¬ëŠ” ê° ëª¨ë¸ë³„ë¡œ ìˆ˜í–‰ (ëª¨ë¸ë³„ë¡œ ì¸ì½”ë”© ë°©ì‹ì´ ë‹¤ë¦„)
    logging.info("ğŸ”§ ëª¨ë¸ë³„ ë°ì´í„° ì „ì²˜ë¦¬ ë° í•™ìŠµ ì‹œì‘...")

    # 3. ë¹ ë¥¸ ëª¨ë¸ë“¤ë§Œ ì„ íƒí•´ì„œ ìˆœì°¨ ì‹¤í–‰
    fast_model_names = ['gradient_boosting', 'extra_trees', 'xgboost', 'random_forest', 
                       'lightgbm', 'logistic_regression', 'naive_bayes']
    successful_models = []
    failed_models = []
    
    logging.info(f"ğŸ¤– ì´ {len(fast_model_names)}ê°œ ë¹ ë¥¸ ëª¨ë¸ í•™ìŠµ ì‹œì‘...")
    logging.info("âš¡ ì‹¤í–‰ ëª¨ë¸: Gradient Boosting, Extra Trees, XGBoost, Random Forest, LightGBM, Logistic Regression, Naive Bayes")
    logging.info("-" * 50)
    
    for i, model_name in enumerate(fast_model_names, 1):
        try:
            logging.info(f"[{i}/{len(fast_model_names)}] {model_name.upper()} ëª¨ë¸ í•™ìŠµ ì‹œì‘...")
            model_start_time = time.time()
            
            # ëª¨ë¸ë³„ ë°ì´í„° ì „ì²˜ë¦¬
            x_tr_processed, x_te_processed = do_preprocessing(
                df_train=x_tr, df_test=x_te, 
                drop_cols=args.drop_cols, encoding_cols=args.encoding_cols, 
                model_name=model_name
            )
            
            # ëª¨ë¸ í•™ìŠµ
            is_model = do_training(df_train=x_tr_processed, df_train_target=y_tr, 
                                 df_test=x_te_processed, df_test_target=y_te, model_name=model_name)
            
            model_time = time.time() - model_start_time
            
            if is_model:
                predictions = is_model.predict_proba(x_te_processed)[:,1]
                successful_models.append(model_name)
                logging.info(f"âœ… {model_name.upper()} ëª¨ë¸ í›ˆë ¨ì´ ëë‚¬ìŠµë‹ˆë‹¤ (ì†Œìš”ì‹œê°„: {model_time:.2f}ì´ˆ)")
            else:
                failed_models.append(model_name)
                logging.info(f"âŒ {model_name.upper()} ëª¨ë¸ í›ˆë ¨ ì‹¤íŒ¨ (ì†Œìš”ì‹œê°„: {model_time:.2f}ì´ˆ)")
                
        except Exception as e:
            model_time = time.time() - model_start_time
            failed_models.append(model_name)
            logging.error(f"âŒ {model_name.upper()} ëª¨ë¸ í›ˆë ¨ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)} (ì†Œìš”ì‹œê°„: {model_time:.2f}ì´ˆ)")
        
        logging.info("-" * 30)
    
    # ì „ì²´ ì‹¤í–‰ ê²°ê³¼ ìš”ì•½
    total_time = time.time() - total_start_time
    logging.info("="*50)
    logging.info("ğŸ‰ ì „ì²´ ëª¨ë¸ ì‹¤í–‰ ì™„ë£Œ!")
    logging.info(f"ì´ ì†Œìš”ì‹œê°„: {total_time:.2f}ì´ˆ")
    logging.info(f"ì„±ê³µí•œ ëª¨ë¸: {len(successful_models)}ê°œ")
    logging.info(f"ì‹¤íŒ¨í•œ ëª¨ë¸: {len(failed_models)}ê°œ")
    
    if successful_models:
        logging.info(f"âœ… ì„±ê³µ ëª¨ë¸ ëª©ë¡: {', '.join([m.upper() for m in successful_models])}")
    
    if failed_models:
        logging.info(f"âŒ ì‹¤íŒ¨ ëª¨ë¸ ëª©ë¡: {', '.join([m.upper() for m in failed_models])}")
    
    logging.info("ğŸ“ ì„±ëŠ¥ ë³´ê³ ì„œëŠ” reports/ í´ë”ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    # ìµœì¢… ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ ë³´ê³ ì„œ ìƒì„±
    if successful_models:
        create_final_comparison_report(successful_models)
    
    logging.info("="*50)



if __name__ == "__main__":
    ###############################
    # ì½”ë“œ ì‹¤í–‰ ì˜ì—­ 
    ###############################
    args = argparse.ArgumentParser() # 
    args.add_argument("--path_train", default="./data/hotel_bookings_train.csv", type=str)
    args.add_argument("--path_test", default="./data/hotel_bookings_test.csv", type=str)
    #args.add_argument("--path_submission", default="./data/submission.csv", type=str)
    args.add_argument("--target_name", default="is_canceled", type=str) 
    # eda ë¶„ì„ ê²°ê³¼
    args.add_argument("--drop_cols", default=[
        'deposit_type', 'company' ,'agent','reservation_status', 'reservation_status_date'
        , 'assigned_room_type', 'children', 'babies', 'arrival_date_full'], type=list)
    # args.add_argument("--transform_cols", default=['adr', 'lead_time', 'total_stays'], type=list)
    args.add_argument("--encoding_cols", default=['hotel', 'arrival_date_month', 'meal', 'country', 'market_segment', 'distribution_channel', 'reserved_room_type', 'customer_type', 'market_risk_level'], type=list)


    main(args.parse_args()) 