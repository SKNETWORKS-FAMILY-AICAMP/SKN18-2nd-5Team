{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 호텔 예약 데이터를 이용한 고객 수요 예측 프로젝트\n",
        "\n",
        "## 프로젝트 개요\n",
        "- **주제**: 호텔 예약 데이터를 이용하여 상황 가설에 따른 고객 수요 예측\n",
        "- **가설 예시**: 오래 전에 예약한 사람의 예약 취소 가능성이 높다\n",
        "- **목표**: 예약 취소 여부를 예측하는 머신러닝 모델 구축\n",
        "\n",
        "## 분석 단계\n",
        "1. 데이터 로드 및 기본 정보 확인\n",
        "2. 탐색적 데이터 분석 (EDA)\n",
        "3. 데이터 전처리\n",
        "4. 피처 엔지니어링\n",
        "5. 모델링 및 하이퍼파라미터 튜닝\n",
        "6. 모델 평가\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 1. 라이브러리 Import 및 기본 설정\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'imblearn'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 28\u001b[39m\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m accuracy_score, precision_score, recall_score, f1_score\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# 불균형 데이터 처리\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mimblearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mover_sampling\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SMOTE\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mimblearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01munder_sampling\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RandomUnderSampler\n\u001b[32m     31\u001b[39m \u001b[38;5;66;03m# 한글 폰트 설정\u001b[39;00m\n",
            "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'imblearn'"
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
            "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
            "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "# 데이터 처리 및 분석\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# 시각화\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "# 통계 분석\n",
        "from scipy import stats\n",
        "from scipy.stats import chi2_contingency\n",
        "\n",
        "# 머신러닝\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.metrics import roc_auc_score, roc_curve\n",
        "\n",
        "\n",
        "print(\"라이브러리 import 완료!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2. 데이터 로드 및 기본 정보 확인\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 데이터 로드\n",
        "df = pd.read_csv('data/hotel_bookings.csv')\n",
        "\n",
        "print(\"=== 데이터셋 기본 정보 ===\")\n",
        "print(f\"데이터셋 크기: {df.shape}\")\n",
        "print(f\"행 개수: {df.shape[0]:,}\")\n",
        "print(f\"열 개수: {df.shape[1]}\")\n",
        "print(\"\\n=== 첫 5행 데이터 ===\")\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 데이터 타입 및 기본 통계 정보\n",
        "print(\"=== 데이터 타입 정보 ===\")\n",
        "print(df.dtypes)\n",
        "print(\"\\n=== 기본 통계 정보 ===\")\n",
        "df.describe()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 결측치 확인\n",
        "print(\"=== 결측치 정보 ===\")\n",
        "missing_data = df.isnull().sum()\n",
        "missing_percentage = (missing_data / len(df)) * 100\n",
        "missing_info = pd.DataFrame({\n",
        "    '결측치 개수': missing_data,\n",
        "    '결측치 비율(%)': missing_percentage\n",
        "}).sort_values('결측치 개수', ascending=False)\n",
        "\n",
        "print(missing_info[missing_info['결측치 개수'] > 0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 타겟 변수 분포 확인 (예약 취소 여부)\n",
        "print(\"=== 타겟 변수 분포 (is_canceled) ===\")\n",
        "target_counts = df['is_canceled'].value_counts()\n",
        "target_percentage = df['is_canceled'].value_counts(normalize=True) * 100\n",
        "\n",
        "print(\"예약 취소 여부 분포:\")\n",
        "for i, (count, pct) in enumerate(zip(target_counts, target_percentage)):\n",
        "    status = \"취소\" if target_counts.index[i] == 1 else \"유지\"\n",
        "    print(f\"{status}: {count:,}건 ({pct:.1f}%)\")\n",
        "\n",
        "# 시각화\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "# 막대 그래프\n",
        "target_counts.plot(kind='bar', ax=ax1, color=['skyblue', 'salmon'])\n",
        "ax1.set_title('예약 취소 여부 분포')\n",
        "ax1.set_xlabel('예약 상태 (0: 유지, 1: 취소)')\n",
        "ax1.set_ylabel('건수')\n",
        "ax1.tick_params(axis='x', rotation=0)\n",
        "\n",
        "# 파이 차트\n",
        "ax2.pie(target_counts.values, labels=['유지', '취소'], autopct='%1.1f%%', \n",
        "        colors=['skyblue', 'salmon'], startangle=90)\n",
        "ax2.set_title('예약 취소 비율')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 3. 탐색적 데이터 분석 (EDA)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3.1 주요 가설 검증: Lead Time과 예약 취소의 관계\n",
        "print(\"=== 가설 검증: 오래 전에 예약한 사람의 예약 취소 가능성 ===\")\n",
        "\n",
        "# Lead Time 기본 통계\n",
        "print(\"Lead Time 기본 통계:\")\n",
        "print(df.groupby('is_canceled')['lead_time'].describe())\n",
        "\n",
        "# Lead Time 구간별 취소율 분석\n",
        "df['lead_time_group'] = pd.cut(df['lead_time'], \n",
        "                              bins=[0, 30, 90, 180, 365, df['lead_time'].max()],\n",
        "                              labels=['0-30일', '31-90일', '91-180일', '181-365일', '365일+'])\n",
        "\n",
        "cancel_by_leadtime = df.groupby('lead_time_group')['is_canceled'].agg(['count', 'sum', 'mean']).round(3)\n",
        "cancel_by_leadtime.columns = ['전체건수', '취소건수', '취소율']\n",
        "print(\"\\nLead Time 구간별 취소율:\")\n",
        "print(cancel_by_leadtime)\n",
        "\n",
        "# 시각화\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "# Lead Time 분포\n",
        "df.boxplot(column='lead_time', by='is_canceled', ax=ax1)\n",
        "ax1.set_title('예약 취소 여부별 Lead Time 분포')\n",
        "ax1.set_xlabel('예약 상태 (0: 유지, 1: 취소)')\n",
        "ax1.set_ylabel('Lead Time (일)')\n",
        "\n",
        "# Lead Time 구간별 취소율\n",
        "cancel_by_leadtime['취소율'].plot(kind='bar', ax=ax2, color='coral')\n",
        "ax2.set_title('Lead Time 구간별 예약 취소율')\n",
        "ax2.set_xlabel('Lead Time 구간')\n",
        "ax2.set_ylabel('취소율')\n",
        "ax2.tick_params(axis='x', rotation=45)\n",
        "ax2.set_ylim(0, 1)\n",
        "\n",
        "# 취소율 값 표시\n",
        "for i, v in enumerate(cancel_by_leadtime['취소율']):\n",
        "    ax2.text(i, v + 0.01, f'{v:.1%}', ha='center', va='bottom')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3.2 주요 변수들과 예약 취소의 관계 분석\n",
        "\n",
        "# 호텔 타입별 취소율\n",
        "print(\"=== 호텔 타입별 취소율 ===\")\n",
        "hotel_cancel = df.groupby('hotel')['is_canceled'].agg(['count', 'sum', 'mean']).round(3)\n",
        "hotel_cancel.columns = ['전체건수', '취소건수', '취소율']\n",
        "print(hotel_cancel)\n",
        "\n",
        "# 시장 세그먼트별 취소율\n",
        "print(\"\\n=== 시장 세그먼트별 취소율 ===\")\n",
        "market_cancel = df.groupby('market_segment')['is_canceled'].agg(['count', 'sum', 'mean']).round(3)\n",
        "market_cancel.columns = ['전체건수', '취소건수', '취소율']\n",
        "print(market_cancel.sort_values('취소율', ascending=False))\n",
        "\n",
        "# 고객 타입별 취소율\n",
        "print(\"\\n=== 고객 타입별 취소율 ===\")\n",
        "customer_cancel = df.groupby('customer_type')['is_canceled'].agg(['count', 'sum', 'mean']).round(3)\n",
        "customer_cancel.columns = ['전체건수', '취소건수', '취소율']\n",
        "print(customer_cancel.sort_values('취소율', ascending=False))\n",
        "\n",
        "# 시각화\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "# 호텔 타입별 취소율\n",
        "hotel_cancel['취소율'].plot(kind='bar', ax=axes[0,0], color='lightblue')\n",
        "axes[0,0].set_title('호텔 타입별 예약 취소율')\n",
        "axes[0,0].set_ylabel('취소율')\n",
        "axes[0,0].tick_params(axis='x', rotation=0)\n",
        "\n",
        "# 시장 세그먼트별 취소율\n",
        "market_cancel['취소율'].plot(kind='bar', ax=axes[0,1], color='lightgreen')\n",
        "axes[0,1].set_title('시장 세그먼트별 예약 취소율')\n",
        "axes[0,1].set_ylabel('취소율')\n",
        "axes[0,1].tick_params(axis='x', rotation=45)\n",
        "\n",
        "# 고객 타입별 취소율\n",
        "customer_cancel['취소율'].plot(kind='bar', ax=axes[1,0], color='lightcoral')\n",
        "axes[1,0].set_title('고객 타입별 예약 취소율')\n",
        "axes[1,0].set_ylabel('취소율')\n",
        "axes[1,0].tick_params(axis='x', rotation=45)\n",
        "\n",
        "# ADR(Average Daily Rate) 분포\n",
        "df.boxplot(column='adr', by='is_canceled', ax=axes[1,1])\n",
        "axes[1,1].set_title('예약 취소 여부별 평균 일일 요금(ADR) 분포')\n",
        "axes[1,1].set_xlabel('예약 상태 (0: 유지, 1: 취소)')\n",
        "axes[1,1].set_ylabel('ADR')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3.3 상관관계 분석\n",
        "print(\"=== 수치형 변수들 간의 상관관계 ===\")\n",
        "\n",
        "# 수치형 변수들 선택\n",
        "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "print(f\"수치형 변수들: {numeric_cols}\")\n",
        "\n",
        "# 상관관계 매트릭스 계산\n",
        "correlation_matrix = df[numeric_cols].corr()\n",
        "\n",
        "# 타겟 변수와의 상관관계\n",
        "target_corr = correlation_matrix['is_canceled'].sort_values(key=abs, ascending=False)\n",
        "print(\"\\n예약 취소와 상관관계가 높은 변수들:\")\n",
        "print(target_corr[1:11])  # 자기 자신 제외하고 상위 10개\n",
        "\n",
        "# 상관관계 히트맵\n",
        "plt.figure(figsize=(14, 10))\n",
        "mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
        "sns.heatmap(correlation_matrix, mask=mask, annot=True, cmap='coolwarm', center=0,\n",
        "            square=True, fmt='.2f', cbar_kws={\"shrink\": .5})\n",
        "plt.title('수치형 변수들 간의 상관관계 히트맵')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 4. 데이터 전처리\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 4.1 데이터 복사본 생성 및 기본 전처리\n",
        "df_processed = df.copy()\n",
        "\n",
        "print(\"=== 전처리 전 데이터 정보 ===\")\n",
        "print(f\"데이터 크기: {df_processed.shape}\")\n",
        "print(f\"결측치 개수: {df_processed.isnull().sum().sum()}\")\n",
        "\n",
        "# 4.1.1 결측치 처리\n",
        "print(\"\\n=== 결측치 처리 ===\")\n",
        "\n",
        "# children 컬럼의 결측치를 0으로 대체 (일반적으로 아이가 없는 경우)\n",
        "if df_processed['children'].isnull().sum() > 0:\n",
        "    df_processed['children'].fillna(0, inplace=True)\n",
        "    print(\"children 결측치를 0으로 대체\")\n",
        "\n",
        "# country 결측치는 'Unknown'으로 대체\n",
        "if df_processed['country'].isnull().sum() > 0:\n",
        "    df_processed['country'].fillna('Unknown', inplace=True)\n",
        "    print(\"country 결측치를 'Unknown'으로 대체\")\n",
        "\n",
        "# agent와 company 결측치는 0으로 대체 (에이전트나 회사를 통하지 않은 예약)\n",
        "if df_processed['agent'].isnull().sum() > 0:\n",
        "    df_processed['agent'].fillna(0, inplace=True)\n",
        "    print(\"agent 결측치를 0으로 대체\")\n",
        "    \n",
        "if df_processed['company'].isnull().sum() > 0:\n",
        "    df_processed['company'].fillna(0, inplace=True)\n",
        "    print(\"company 결측치를 0으로 대체\")\n",
        "\n",
        "print(f\"전처리 후 결측치 개수: {df_processed.isnull().sum().sum()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 4.1.2 중복 데이터 확인 및 제거\n",
        "print(\"=== 중복 데이터 처리 ===\")\n",
        "duplicate_count = df_processed.duplicated().sum()\n",
        "print(f\"중복 데이터 개수: {duplicate_count}\")\n",
        "\n",
        "if duplicate_count > 0:\n",
        "    df_processed.drop_duplicates(inplace=True)\n",
        "    print(f\"중복 제거 후 데이터 크기: {df_processed.shape}\")\n",
        "else:\n",
        "    print(\"중복 데이터가 없습니다.\")\n",
        "\n",
        "# 4.1.3 이상치 확인 및 처리\n",
        "print(\"\\n=== 이상치 확인 ===\")\n",
        "\n",
        "# ADR (Average Daily Rate) 이상치 확인\n",
        "print(\"ADR 이상치 확인:\")\n",
        "adr_q1 = df_processed['adr'].quantile(0.25)\n",
        "adr_q3 = df_processed['adr'].quantile(0.75)\n",
        "adr_iqr = adr_q3 - adr_q1\n",
        "adr_lower = adr_q1 - 1.5 * adr_iqr\n",
        "adr_upper = adr_q3 + 1.5 * adr_iqr\n",
        "\n",
        "print(f\"ADR Q1: {adr_q1:.2f}, Q3: {adr_q3:.2f}\")\n",
        "print(f\"ADR 정상 범위: {adr_lower:.2f} ~ {adr_upper:.2f}\")\n",
        "\n",
        "adr_outliers = df_processed[(df_processed['adr'] < adr_lower) | (df_processed['adr'] > adr_upper)]\n",
        "print(f\"ADR 이상치 개수: {len(adr_outliers)} ({len(adr_outliers)/len(df_processed)*100:.1f}%)\")\n",
        "\n",
        "# 음수 ADR 확인 (비현실적인 값)\n",
        "negative_adr = df_processed[df_processed['adr'] < 0]\n",
        "print(f\"음수 ADR 개수: {len(negative_adr)}\")\n",
        "\n",
        "# 음수 ADR 제거\n",
        "if len(negative_adr) > 0:\n",
        "    df_processed = df_processed[df_processed['adr'] >= 0]\n",
        "    print(f\"음수 ADR 제거 후 데이터 크기: {df_processed.shape}\")\n",
        "\n",
        "# lead_time 이상치 확인\n",
        "print(f\"\\nLead Time 통계:\")\n",
        "print(f\"최솟값: {df_processed['lead_time'].min()}\")\n",
        "print(f\"최댓값: {df_processed['lead_time'].max()}\")\n",
        "print(f\"평균: {df_processed['lead_time'].mean():.1f}\")\n",
        "print(f\"중앙값: {df_processed['lead_time'].median():.1f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 4.1.4 왜도(Skewness)와 첨도(Kurtosis) 분석\n",
        "print(\"=== 왜도와 첨도 분석 ===\")\n",
        "\n",
        "numeric_cols = df_processed.select_dtypes(include=[np.number]).columns.tolist()\n",
        "skew_kurt_analysis = pd.DataFrame({\n",
        "    '변수명': numeric_cols,\n",
        "    '왜도': [df_processed[col].skew() for col in numeric_cols],\n",
        "    '첨도': [df_processed[col].kurtosis() for col in numeric_cols]\n",
        "})\n",
        "\n",
        "# 왜도와 첨도가 높은 변수들 확인\n",
        "skew_kurt_analysis['왜도_절댓값'] = abs(skew_kurt_analysis['왜도'])\n",
        "skew_kurt_analysis = skew_kurt_analysis.sort_values('왜도_절댓값', ascending=False)\n",
        "\n",
        "print(\"변수별 왜도와 첨도:\")\n",
        "print(skew_kurt_analysis[['변수명', '왜도', '첨도']].head(10))\n",
        "\n",
        "# 왜도가 높은 변수들 시각화\n",
        "high_skew_vars = skew_kurt_analysis[skew_kurt_analysis['왜도_절댓값'] > 2]['변수명'].head(4).tolist()\n",
        "\n",
        "if high_skew_vars:\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "    axes = axes.ravel()\n",
        "    \n",
        "    for i, var in enumerate(high_skew_vars):\n",
        "        if i < 4:\n",
        "            df_processed[var].hist(bins=50, ax=axes[i], alpha=0.7)\n",
        "            axes[i].set_title(f'{var} 분포 (왜도: {df_processed[var].skew():.2f})')\n",
        "            axes[i].set_ylabel('빈도')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "print(f\"\\n왜도가 2 이상인 변수들: {len(skew_kurt_analysis[skew_kurt_analysis['왜도_절댓값'] > 2])}개\")\n",
        "print(f\"첨도가 5 이상인 변수들: {len(skew_kurt_analysis[abs(skew_kurt_analysis['첨도']) > 5])}개\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 5. 피처 엔지니어링\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 5.1 새로운 피처 생성\n",
        "print(\"=== 새로운 피처 생성 ===\")\n",
        "\n",
        "# 전체 숙박 일수\n",
        "df_processed['total_nights'] = df_processed['stays_in_weekend_nights'] + df_processed['stays_in_week_nights']\n",
        "\n",
        "# 전체 손님 수\n",
        "df_processed['total_guests'] = df_processed['adults'] + df_processed['children'] + df_processed['babies']\n",
        "\n",
        "# 1인당 평균 요금\n",
        "df_processed['adr_per_person'] = df_processed['adr'] / df_processed['total_guests'].replace(0, 1)  # 0으로 나누기 방지\n",
        "\n",
        "# 예약 변경 여부\n",
        "df_processed['has_booking_changes'] = (df_processed['booking_changes'] > 0).astype(int)\n",
        "\n",
        "# 특별 요청 여부\n",
        "df_processed['has_special_requests'] = (df_processed['total_of_special_requests'] > 0).astype(int)\n",
        "\n",
        "# 이전 취소 이력 여부\n",
        "df_processed['has_previous_cancellations'] = (df_processed['previous_cancellations'] > 0).astype(int)\n",
        "\n",
        "# 이전 예약 이력 여부\n",
        "df_processed['has_previous_bookings'] = (df_processed['previous_bookings_not_canceled'] > 0).astype(int)\n",
        "\n",
        "# Lead time 카테고리\n",
        "df_processed['lead_time_category'] = pd.cut(df_processed['lead_time'], \n",
        "                                          bins=[0, 7, 30, 90, 365, df_processed['lead_time'].max()],\n",
        "                                          labels=['매우짧음(0-7일)', '짧음(8-30일)', '보통(31-90일)', '김(91-365일)', '매우김(365일+)'])\n",
        "\n",
        "# 도착 월을 계절로 분류\n",
        "season_map = {\n",
        "    'January': '겨울', 'February': '겨울', 'March': '봄',\n",
        "    'April': '봄', 'May': '봄', 'June': '여름',\n",
        "    'July': '여름', 'August': '여름', 'September': '가을',\n",
        "    'October': '가을', 'November': '가을', 'December': '겨울'\n",
        "}\n",
        "df_processed['season'] = df_processed['arrival_date_month'].map(season_map)\n",
        "\n",
        "# 주말 포함 여부\n",
        "df_processed['includes_weekend'] = (df_processed['stays_in_weekend_nights'] > 0).astype(int)\n",
        "\n",
        "print(\"생성된 새로운 피처들:\")\n",
        "new_features = ['total_nights', 'total_guests', 'adr_per_person', 'has_booking_changes', \n",
        "                'has_special_requests', 'has_previous_cancellations', 'has_previous_bookings',\n",
        "                'lead_time_category', 'season', 'includes_weekend']\n",
        "for feature in new_features:\n",
        "    print(f\"- {feature}\")\n",
        "\n",
        "print(f\"\\n전체 피처 개수: {len(df_processed.columns)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 5.2 범주형 변수 인코딩\n",
        "print(\"=== 범주형 변수 인코딩 ===\")\n",
        "\n",
        "# 모델링을 위한 데이터프레임 복사\n",
        "df_model = df_processed.copy()\n",
        "\n",
        "# 범주형 변수들 확인\n",
        "categorical_cols = df_model.select_dtypes(include=['object']).columns.tolist()\n",
        "print(f\"범주형 변수들: {categorical_cols}\")\n",
        "\n",
        "# 불필요한 컬럼 제거 (모델링에 직접적으로 사용하지 않을 컬럼들)\n",
        "cols_to_drop = ['reservation_status_date', 'arrival_date_year', 'arrival_date_month', \n",
        "                'arrival_date_week_number', 'arrival_date_day_of_month', 'lead_time_group']\n",
        "\n",
        "# 존재하는 컬럼만 제거\n",
        "cols_to_drop = [col for col in cols_to_drop if col in df_model.columns]\n",
        "df_model.drop(columns=cols_to_drop, inplace=True)\n",
        "print(f\"제거된 컬럼들: {cols_to_drop}\")\n",
        "\n",
        "# 범주형 변수들 다시 확인\n",
        "categorical_cols = df_model.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "print(f\"인코딩할 범주형 변수들: {categorical_cols}\")\n",
        "\n",
        "# Label Encoding을 적용할 변수들 (순서가 있는 변수들)\n",
        "ordinal_cols = ['lead_time_category']\n",
        "\n",
        "# One-Hot Encoding을 적용할 변수들 (순서가 없는 변수들)\n",
        "nominal_cols = [col for col in categorical_cols if col not in ordinal_cols]\n",
        "\n",
        "print(f\"Label Encoding 적용: {ordinal_cols}\")\n",
        "print(f\"One-Hot Encoding 적용: {nominal_cols}\")\n",
        "\n",
        "# Label Encoding\n",
        "label_encoders = {}\n",
        "for col in ordinal_cols:\n",
        "    if col in df_model.columns:\n",
        "        le = LabelEncoder()\n",
        "        df_model[col] = le.fit_transform(df_model[col].astype(str))\n",
        "        label_encoders[col] = le\n",
        "        print(f\"{col} Label Encoding 완료\")\n",
        "\n",
        "# One-Hot Encoding (카디널리티가 너무 높지 않은 변수들만)\n",
        "for col in nominal_cols:\n",
        "    if col in df_model.columns:\n",
        "        unique_count = df_model[col].nunique()\n",
        "        print(f\"{col} 고유값 개수: {unique_count}\")\n",
        "        \n",
        "        if unique_count <= 50:  # 카디널리티가 50 이하인 경우만 One-Hot Encoding\n",
        "            dummies = pd.get_dummies(df_model[col], prefix=col, drop_first=True)\n",
        "            df_model = pd.concat([df_model, dummies], axis=1)\n",
        "            df_model.drop(columns=[col], inplace=True)\n",
        "            print(f\"{col} One-Hot Encoding 완료\")\n",
        "        else:\n",
        "            # 카디널리티가 높은 경우 빈도 기반 인코딩\n",
        "            freq_encoding = df_model[col].value_counts().to_dict()\n",
        "            df_model[col + '_freq'] = df_model[col].map(freq_encoding)\n",
        "            df_model.drop(columns=[col], inplace=True)\n",
        "            print(f\"{col} 빈도 기반 인코딩 완료\")\n",
        "\n",
        "print(f\"\\n인코딩 후 피처 개수: {len(df_model.columns)}\")\n",
        "print(f\"인코딩 후 데이터 크기: {df_model.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 5.3 피처 스케일링\n",
        "print(\"=== 피처 스케일링 ===\")\n",
        "\n",
        "# 타겟 변수 분리\n",
        "X = df_model.drop('is_canceled', axis=1)\n",
        "y = df_model['is_canceled']\n",
        "\n",
        "print(f\"피처 개수: {X.shape[1]}\")\n",
        "print(f\"샘플 개수: {X.shape[0]}\")\n",
        "print(f\"타겟 변수 분포:\\n{y.value_counts()}\")\n",
        "\n",
        "# 수치형 변수들 확인\n",
        "numeric_features = X.select_dtypes(include=[np.number]).columns.tolist()\n",
        "print(f\"수치형 피처 개수: {len(numeric_features)}\")\n",
        "\n",
        "# 학습/테스트 데이터 분할\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "print(f\"학습 데이터 크기: {X_train.shape}\")\n",
        "print(f\"테스트 데이터 크기: {X_test.shape}\")\n",
        "print(f\"학습 데이터 타겟 분포:\\n{y_train.value_counts()}\")\n",
        "\n",
        "# StandardScaler를 사용하여 수치형 피처들 스케일링\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = X_train.copy()\n",
        "X_test_scaled = X_test.copy()\n",
        "\n",
        "# 수치형 피처들만 스케일링\n",
        "if numeric_features:\n",
        "    X_train_scaled[numeric_features] = scaler.fit_transform(X_train[numeric_features])\n",
        "    X_test_scaled[numeric_features] = scaler.transform(X_test[numeric_features])\n",
        "    print(\"수치형 피처들 스케일링 완료\")\n",
        "\n",
        "print(f\"스케일링 후 학습 데이터 통계:\")\n",
        "print(X_train_scaled[numeric_features[:5]].describe().round(3))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 5.4 불균형 데이터 처리 (옵션)\n",
        "print(\"=== 불균형 데이터 확인 및 처리 ===\")\n",
        "\n",
        "# 타겟 변수 불균형 확인\n",
        "target_ratio = y_train.value_counts(normalize=True)\n",
        "print(\"타겟 변수 비율:\")\n",
        "print(f\"예약 유지 (0): {target_ratio[0]:.1%}\")\n",
        "print(f\"예약 취소 (1): {target_ratio[1]:.1%}\")\n",
        "\n",
        "minority_ratio = min(target_ratio[0], target_ratio[1])\n",
        "print(f\"소수 클래스 비율: {minority_ratio:.1%}\")\n",
        "\n",
        "# 불균형이 심한 경우 (소수 클래스가 30% 미만) SMOTE 적용\n",
        "if minority_ratio < 0.3:\n",
        "    print(\"불균형 데이터 감지 - SMOTE 적용\")\n",
        "    \n",
        "    # SMOTE 적용\n",
        "    smote = SMOTE(random_state=42, k_neighbors=5)\n",
        "    X_train_balanced, y_train_balanced = smote.fit_resample(X_train_scaled, y_train)\n",
        "    \n",
        "    print(f\"SMOTE 적용 전 학습 데이터 크기: {X_train_scaled.shape}\")\n",
        "    print(f\"SMOTE 적용 후 학습 데이터 크기: {X_train_balanced.shape}\")\n",
        "    \n",
        "    balanced_ratio = y_train_balanced.value_counts(normalize=True)\n",
        "    print(\"SMOTE 적용 후 타겟 변수 비율:\")\n",
        "    print(f\"예약 유지 (0): {balanced_ratio[0]:.1%}\")\n",
        "    print(f\"예약 취소 (1): {balanced_ratio[1]:.1%}\")\n",
        "    \n",
        "    # 균형 맞춘 데이터를 기본으로 사용\n",
        "    X_train_final = X_train_balanced\n",
        "    y_train_final = y_train_balanced\n",
        "    \n",
        "else:\n",
        "    print(\"데이터 균형이 적절함 - 원본 데이터 사용\")\n",
        "    X_train_final = X_train_scaled\n",
        "    y_train_final = y_train\n",
        "\n",
        "print(f\"최종 학습 데이터 크기: {X_train_final.shape}\")\n",
        "print(f\"최종 테스트 데이터 크기: {X_test_scaled.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 6. 모델링 및 하이퍼파라미터 튜닝\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 6.1 기본 모델들 성능 비교\n",
        "print(\"=== 기본 모델들 성능 비교 ===\")\n",
        "\n",
        "# 여러 모델들 정의\n",
        "models = {\n",
        "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
        "    'Random Forest': RandomForestClassifier(random_state=42, n_estimators=100),\n",
        "    'Gradient Boosting': GradientBoostingClassifier(random_state=42, n_estimators=100),\n",
        "    'SVM': SVC(random_state=42, probability=True)\n",
        "}\n",
        "\n",
        "# 교차 검증을 위한 설정\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# 모델별 성능 저장\n",
        "model_scores = {}\n",
        "\n",
        "print(\"교차 검증 진행 중...\")\n",
        "for name, model in models.items():\n",
        "    print(f\"\\n{name} 학습 중...\")\n",
        "    \n",
        "    # 교차 검증\n",
        "    cv_scores = cross_val_score(model, X_train_final, y_train_final, cv=cv, scoring='roc_auc', n_jobs=-1)\n",
        "    \n",
        "    model_scores[name] = {\n",
        "        'CV Mean': cv_scores.mean(),\n",
        "        'CV Std': cv_scores.std(),\n",
        "        'CV Scores': cv_scores\n",
        "    }\n",
        "    \n",
        "    print(f\"{name} - AUC: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")\n",
        "\n",
        "# 결과 정리\n",
        "results_df = pd.DataFrame({\n",
        "    'Model': list(model_scores.keys()),\n",
        "    'CV_Mean_AUC': [scores['CV Mean'] for scores in model_scores.values()],\n",
        "    'CV_Std_AUC': [scores['CV Std'] for scores in model_scores.values()]\n",
        "}).sort_values('CV_Mean_AUC', ascending=False)\n",
        "\n",
        "print(\"\\n=== 모델 성능 순위 ===\")\n",
        "print(results_df)\n",
        "\n",
        "# 시각화\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.bar(results_df['Model'], results_df['CV_Mean_AUC'], yerr=results_df['CV_Std_AUC'], capsize=5)\n",
        "plt.title('모델별 교차검증 AUC 점수')\n",
        "plt.ylabel('AUC Score')\n",
        "plt.xticks(rotation=45)\n",
        "plt.ylim(0.8, 1.0)\n",
        "\n",
        "# 박스플롯\n",
        "plt.subplot(1, 2, 2)\n",
        "cv_data = [model_scores[name]['CV Scores'] for name in results_df['Model']]\n",
        "plt.boxplot(cv_data, labels=results_df['Model'])\n",
        "plt.title('모델별 교차검증 AUC 분포')\n",
        "plt.ylabel('AUC Score')\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 6.2 최고 성능 모델 하이퍼파라미터 튜닝\n",
        "print(\"=== 하이퍼파라미터 튜닝 ===\")\n",
        "\n",
        "# 가장 성능이 좋은 모델 선택\n",
        "best_model_name = results_df.iloc[0]['Model']\n",
        "print(f\"최고 성능 모델: {best_model_name}\")\n",
        "\n",
        "# Random Forest 하이퍼파라미터 튜닝 (일반적으로 좋은 성능을 보이는 모델)\n",
        "if 'Random Forest' in best_model_name or True:  # 예시로 Random Forest 튜닝\n",
        "    print(\"Random Forest 하이퍼파라미터 튜닝 진행...\")\n",
        "    \n",
        "    # 하이퍼파라미터 그리드 정의\n",
        "    param_grid = {\n",
        "        'n_estimators': [100, 200],\n",
        "        'max_depth': [10, 20, None],\n",
        "        'min_samples_split': [2, 5],\n",
        "        'min_samples_leaf': [1, 2],\n",
        "        'max_features': ['sqrt', 'log2']\n",
        "    }\n",
        "    \n",
        "    # GridSearchCV 설정\n",
        "    rf = RandomForestClassifier(random_state=42)\n",
        "    grid_search = GridSearchCV(\n",
        "        rf, param_grid, cv=3, scoring='roc_auc', \n",
        "        n_jobs=-1, verbose=1, return_train_score=True\n",
        "    )\n",
        "    \n",
        "    # 하이퍼파라미터 튜닝 실행\n",
        "    print(\"GridSearch 실행 중... (시간이 다소 걸릴 수 있습니다)\")\n",
        "    grid_search.fit(X_train_final, y_train_final)\n",
        "    \n",
        "    # 최적 파라미터 출력\n",
        "    print(f\"\\n최적 파라미터: {grid_search.best_params_}\")\n",
        "    print(f\"최적 교차검증 AUC: {grid_search.best_score_:.4f}\")\n",
        "    \n",
        "    # 최적 모델 저장\n",
        "    best_model = grid_search.best_estimator_\n",
        "    \n",
        "else:\n",
        "    # 다른 모델의 경우 기본 파라미터 사용\n",
        "    if 'Logistic' in best_model_name:\n",
        "        best_model = LogisticRegression(random_state=42, max_iter=1000)\n",
        "    elif 'Gradient' in best_model_name:\n",
        "        best_model = GradientBoostingClassifier(random_state=42, n_estimators=200)\n",
        "    else:\n",
        "        best_model = RandomForestClassifier(random_state=42, n_estimators=200)\n",
        "    \n",
        "    best_model.fit(X_train_final, y_train_final)\n",
        "\n",
        "print(\"하이퍼파라미터 튜닝 완료!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 7. 모델 평가\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 7.1 테스트 데이터 예측 및 성능 평가\n",
        "print(\"=== 최종 모델 성능 평가 ===\")\n",
        "\n",
        "# 테스트 데이터 예측\n",
        "y_pred = best_model.predict(X_test_scaled)\n",
        "y_pred_proba = best_model.predict_proba(X_test_scaled)[:, 1]\n",
        "\n",
        "# 성능 지표 계산\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "auc = roc_auc_score(y_test, y_pred_proba)\n",
        "\n",
        "print(\"=== 성능 지표 ===\")\n",
        "print(f\"정확도 (Accuracy): {accuracy:.4f}\")\n",
        "print(f\"정밀도 (Precision): {precision:.4f}\")\n",
        "print(f\"재현율 (Recall): {recall:.4f}\")\n",
        "print(f\"F1-Score: {f1:.4f}\")\n",
        "print(f\"AUC-ROC: {auc:.4f}\")\n",
        "\n",
        "# 혼동 행렬\n",
        "print(\"\\n=== 혼동 행렬 ===\")\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(cm)\n",
        "\n",
        "# 분류 리포트\n",
        "print(\"\\n=== 분류 리포트 ===\")\n",
        "print(classification_report(y_test, y_pred, target_names=['예약유지', '예약취소']))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 7.2 시각화를 통한 성능 분석\n",
        "print(\"=== 성능 시각화 ===\")\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "\n",
        "# 1. 혼동 행렬 히트맵\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0,0],\n",
        "            xticklabels=['예약유지', '예약취소'], yticklabels=['예약유지', '예약취소'])\n",
        "axes[0,0].set_title('혼동 행렬 (Confusion Matrix)')\n",
        "axes[0,0].set_ylabel('실제값')\n",
        "axes[0,0].set_xlabel('예측값')\n",
        "\n",
        "# 2. ROC 곡선\n",
        "fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
        "axes[0,1].plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {auc:.4f})')\n",
        "axes[0,1].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "axes[0,1].set_xlim([0.0, 1.0])\n",
        "axes[0,1].set_ylim([0.0, 1.05])\n",
        "axes[0,1].set_xlabel('False Positive Rate')\n",
        "axes[0,1].set_ylabel('True Positive Rate')\n",
        "axes[0,1].set_title('ROC 곡선')\n",
        "axes[0,1].legend(loc=\"lower right\")\n",
        "\n",
        "# 3. 예측 확률 분포\n",
        "axes[1,0].hist(y_pred_proba[y_test==0], bins=50, alpha=0.7, label='예약유지', color='skyblue')\n",
        "axes[1,0].hist(y_pred_proba[y_test==1], bins=50, alpha=0.7, label='예약취소', color='salmon')\n",
        "axes[1,0].set_xlabel('예측 확률')\n",
        "axes[1,0].set_ylabel('빈도')\n",
        "axes[1,0].set_title('클래스별 예측 확률 분포')\n",
        "axes[1,0].legend()\n",
        "\n",
        "# 4. 성능 지표 바 차트\n",
        "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'AUC-ROC']\n",
        "values = [accuracy, precision, recall, f1, auc]\n",
        "bars = axes[1,1].bar(metrics, values, color=['lightblue', 'lightgreen', 'lightcoral', 'lightyellow', 'lightpink'])\n",
        "axes[1,1].set_title('성능 지표 요약')\n",
        "axes[1,1].set_ylabel('점수')\n",
        "axes[1,1].set_ylim(0, 1)\n",
        "\n",
        "# 바 위에 값 표시\n",
        "for bar, value in zip(bars, values):\n",
        "    axes[1,1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
        "                   f'{value:.3f}', ha='center', va='bottom')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 7.3 피처 중요도 분석\n",
        "print(\"=== 피처 중요도 분석 ===\")\n",
        "\n",
        "# Random Forest의 경우 피처 중요도 확인 가능\n",
        "if hasattr(best_model, 'feature_importances_'):\n",
        "    # 피처 중요도 추출\n",
        "    feature_importance = pd.DataFrame({\n",
        "        'feature': X_train_final.columns,\n",
        "        'importance': best_model.feature_importances_\n",
        "    }).sort_values('importance', ascending=False)\n",
        "    \n",
        "    print(\"상위 20개 중요 피처:\")\n",
        "    print(feature_importance.head(20))\n",
        "    \n",
        "    # 피처 중요도 시각화\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    top_features = feature_importance.head(15)\n",
        "    plt.barh(range(len(top_features)), top_features['importance'])\n",
        "    plt.yticks(range(len(top_features)), top_features['feature'])\n",
        "    plt.xlabel('중요도')\n",
        "    plt.title('상위 15개 피처 중요도')\n",
        "    plt.gca().invert_yaxis()\n",
        "    \n",
        "    # 중요도 값 표시\n",
        "    for i, v in enumerate(top_features['importance']):\n",
        "        plt.text(v + 0.001, i, f'{v:.3f}', va='center')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "else:\n",
        "    print(\"현재 모델은 피처 중요도를 제공하지 않습니다.\")\n",
        "\n",
        "# 7.4 Overfitting/Underfitting 검증\n",
        "print(\"\\n=== Overfitting/Underfitting 검증 ===\")\n",
        "\n",
        "# 학습 데이터에 대한 예측\n",
        "y_train_pred = best_model.predict(X_train_final)\n",
        "y_train_pred_proba = best_model.predict_proba(X_train_final)[:, 1]\n",
        "\n",
        "# 학습 데이터 성능\n",
        "train_accuracy = accuracy_score(y_train_final, y_train_pred)\n",
        "train_auc = roc_auc_score(y_train_final, y_train_pred_proba)\n",
        "\n",
        "print(f\"학습 데이터 정확도: {train_accuracy:.4f}\")\n",
        "print(f\"테스트 데이터 정확도: {accuracy:.4f}\")\n",
        "print(f\"정확도 차이: {train_accuracy - accuracy:.4f}\")\n",
        "\n",
        "print(f\"\\n학습 데이터 AUC: {train_auc:.4f}\")\n",
        "print(f\"테스트 데이터 AUC: {auc:.4f}\")\n",
        "print(f\"AUC 차이: {train_auc - auc:.4f}\")\n",
        "\n",
        "# 판정\n",
        "if train_auc - auc > 0.05:\n",
        "    print(\"\\n⚠️ Overfitting 의심: 학습 데이터와 테스트 데이터 성능 차이가 큽니다.\")\n",
        "elif auc < 0.7:\n",
        "    print(\"\\n⚠️ Underfitting 의심: 전체적인 성능이 낮습니다.\")\n",
        "else:\n",
        "    print(\"\\n✅ 적절한 학습: 과적합이나 과소적합 없이 잘 학습되었습니다.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
